# -*- coding: utf-8 -*-
import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, accuracy_score
import joblib

# ==========================================
# ğŸ“ è·¯å¾„
# ==========================================
os.chdir(os.path.dirname(os.path.abspath(__file__)))

DATA_INDEX = "n_rebound_dataset.csv"
RAW_DATA_DIR = "training_data"
MODEL_SAVE_PATH = "n_rebound_xgb.model"


def load_data_fast(csv_path):
    """å¿«é€ŸåŠ è½½å¹¶æ„é€ ç‰¹å¾"""
    if not os.path.exists(csv_path):
        print("âŒ æ‰¾ä¸åˆ°æ•°æ®é›†ç´¢å¼•æ–‡ä»¶")
        return None, None, None

    df_index = pd.read_csv(csv_path)
    print(f"ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ® (å…± {len(df_index)} æ¡)...")

    X_data = []
    y_data = []

    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    file_cache = {}
    if os.path.exists(RAW_DATA_DIR):
        for f in os.listdir(RAW_DATA_DIR):
            if f.endswith(".csv"):
                # å…¼å®¹ä¸åŒå‘½åå‰ç¼€
                clean_code = f.replace(".csv", "").replace("sh", "").replace("sz", "")
                file_cache[clean_code] = os.path.join(RAW_DATA_DIR, f)

    valid_count = 0

    for _, row in df_index.iterrows():
        code = str(row['code']).zfill(6)
        buy_date = row['buy_date']
        label = int(row['label'])

        if code not in file_cache: continue

        try:
            df = pd.read_csv(file_cache[code])
            col_map = {'date': 'æ—¥æœŸ', 'open': 'å¼€ç›˜', 'close': 'æ”¶ç›˜', 'high': 'æœ€é«˜', 'low': 'æœ€ä½',
                       'volume': 'æˆäº¤é‡'}
            df.rename(columns=col_map, inplace=True)
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])

            mask = df['æ—¥æœŸ'] == pd.to_datetime(buy_date)
            if not mask.any(): continue
            idx = df.index[mask][0]

            if idx < 29: continue

            # å–30å¤©æ•°æ®
            slice_df = df.iloc[idx - 29: idx + 1].copy()
            vals = slice_df[['æ”¶ç›˜', 'æˆäº¤é‡']].values

            # --- ç‰¹å¾å·¥ç¨‹ ---
            close_prices = vals[:, 0]
            vols = vals[:, 1]

            # 1. æ¶¨å¹…ç‰¹å¾
            p_change_5 = (close_prices[-1] - close_prices[-5]) / (close_prices[-5] + 1e-6)
            p_change_10 = (close_prices[-1] - close_prices[-10]) / (close_prices[-10] + 1e-6)
            p_change_30 = (close_prices[-1] - close_prices[0]) / (close_prices[0] + 1e-6)

            # 2. æ³¢åŠ¨ç‡
            volatility = np.std(close_prices[-5:]) / (np.mean(close_prices[-5:]) + 1e-6)

            # 3. é‡æ¯”
            vol_ratio_5 = vols[-1] / (np.mean(vols[-5:]) + 1e-6)

            # 4. å‡çº¿åç¦»åº¦
            ma5 = np.mean(close_prices[-5:])
            ma20 = np.mean(close_prices[-20:])
            dist_ma5 = (close_prices[-1] / (ma5 + 1e-6)) - 1
            dist_ma20 = (close_prices[-1] / (ma20 + 1e-6)) - 1

            feature = [p_change_5, p_change_10, p_change_30, volatility, vol_ratio_5, dist_ma5, dist_ma20]

            X_data.append(feature)
            y_data.append(label)
            valid_count += 1

            if valid_count % 2000 == 0:
                print(f"\r  å·²å¤„ç† {valid_count} æ¡...", end="")

        except:
            continue

    print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæ¯•! æœ‰æ•ˆæ ·æœ¬: {len(X_data)}")
    feature_names = ['5æ—¥æ¶¨å¹…', '10æ—¥æ¶¨å¹…', '30æ—¥æ¶¨å¹…', 'æ³¢åŠ¨ç‡', 'é‡æ¯”', 'åç¦»MA5', 'åç¦»MA20']
    return np.array(X_data), np.array(y_data), feature_names


def main():
    print("ğŸš€ å¯åŠ¨ XGBoost è®­ç»ƒ (ä¿®å¤ç‰ˆ)")

    X, y, feat_names = load_data_fast(DATA_INDEX)
    if X is None or len(X) == 0:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")
        return

    # åˆ’åˆ†æ•°æ®
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # è‡ªåŠ¨è®¡ç®—æƒé‡
    pos_count = np.sum(y == 1)
    neg_count = np.sum(y == 0)
    pos_ratio = neg_count / (pos_count + 1e-6)
    print(f"âš–ï¸ æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{neg_count / pos_count:.2f} | scale_pos_weight: {pos_ratio:.2f}")

    # --- ğŸ”¥ æ ¸å¿ƒä¿®å¤ ---
    model = xgb.XGBClassifier(
        n_estimators=500,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=pos_ratio,
        eval_metric='logloss',
        early_stopping_rounds=50,  # ğŸ‘ˆ ç§»åˆ°è¿™é‡Œäº†
        n_jobs=-1
    )

    print("\nğŸŒ² å¼€å§‹ç§æ ‘ (Training)...")
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=False
        # fit å‡½æ•°é‡Œä¸éœ€è¦ early_stopping_rounds äº†
    )

    # éªŒè¯
    preds = model.predict(X_val)
    precision = precision_score(y_val, preds, zero_division=0)
    recall = recall_score(y_val, preds, zero_division=0)
    acc = accuracy_score(y_val, preds)

    print("\n" + "=" * 40)
    print("       ğŸ† æœ€ç»ˆæˆ˜æŠ¥ (XGBoost)")
    print("=" * 40)
    print(f"âœ… æŸ¥å‡†ç‡ (Precision): {precision * 100:.2f}%")
    print(f"ğŸ¯ å¬å›ç‡ (Recall):    {recall * 100:.2f}%")
    print(f"ğŸ“Š å‡†ç¡®ç‡ (Accuracy):  {acc * 100:.2f}%")

    # ç‰¹å¾é‡è¦æ€§
    print("\nğŸ” AI æœ€çœ‹é‡ä»€ä¹ˆæŒ‡æ ‡?")
    print("-" * 40)
    importance = model.feature_importances_
    indices = np.argsort(importance)[::-1]

    for f in range(X.shape[1]):
        print(f"{f + 1}. {feat_names[indices[f]]:<10} : {importance[indices[f]]:.4f}")
    print("-" * 40)

    # ä¿å­˜
    if precision > 0.5:
        joblib.dump(model, MODEL_SAVE_PATH)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {MODEL_SAVE_PATH}")
    else:
        print("âš ï¸ æŸ¥å‡†ç‡ä¸è¶³ 50%ï¼Œæ¨¡å‹æ•ˆæœä¸ä½³ã€‚")


if __name__ == "__main__":
    main()