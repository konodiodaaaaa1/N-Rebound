import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import math
import time
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score
from concurrent.futures import ThreadPoolExecutor

# ==========================================
# ğŸ“ æ€§èƒ½é…ç½®åŒº (é’ˆå¯¹ RTX 4070 ä¼˜åŒ–)
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 2048  # ğŸš€ æ¿€è¿›ä¼˜åŒ–
EPOCHS = 100
LEARNING_RATE = 0.001
LOOKBACK_WINDOW = 30
FEATURE_SIZE = 5

DATA_INDEX = "n_rebound_dataset.csv"
RAW_DATA_DIR = "training_data"
MODEL_SAVE_PATH = "n_rebound_model.pth"


# ==========================================
# ğŸ› ï¸ 1. æé€Ÿæ•°æ®é›† (å¸¦å†…å­˜ç¼“å­˜) - ä¿æŒä¸å˜
# ==========================================
class CachedStockDataset(Dataset):
    def __init__(self, index_df, root_dir):
        self.index_df = index_df
        self.root_dir = root_dir
        self.cache = {}  # ğŸ§  å†…å­˜ç¼“å­˜

        print(f"ğŸ”¥ æ­£åœ¨é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜ (å…± {len(index_df)} ä¸ªæ ·æœ¬)...")
        self._preload_data()

    def _preload_data(self):
        unique_codes = self.index_df['code'].unique()

        def load_one_stock(code):
            code_str = str(code).zfill(6)
            # å…¼å®¹å¤šç§æ–‡ä»¶åæ ¼å¼
            possible_names = [f"{code_str}.csv", f"sh{code_str}.csv", f"sz{code_str}.csv"]
            csv_path = None
            for name in possible_names:
                p = os.path.join(self.root_dir, name)
                if os.path.exists(p):
                    csv_path = p
                    break

            if not csv_path: return code, None

            try:
                df = pd.read_csv(csv_path)
                col_map = {'date': 'æ—¥æœŸ', 'open': 'å¼€ç›˜', 'close': 'æ”¶ç›˜', 'high': 'æœ€é«˜', 'low': 'æœ€ä½',
                           'volume': 'æˆäº¤é‡'}
                df.rename(columns=col_map, inplace=True)
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
                return code, df
            except:
                return code, None

        with ThreadPoolExecutor(max_workers=8) as executor:
            results = executor.map(load_one_stock, unique_codes)

        for code, df in results:
            if df is not None:
                self.cache[code] = df

        print(f"âœ… é¢„åŠ è½½å®Œæˆï¼ç¼“å­˜äº† {len(self.cache)} åªè‚¡ç¥¨çš„æ•°æ®ã€‚")

    def __len__(self):
        return len(self.index_df)

    def __getitem__(self, idx):
        row = self.index_df.iloc[idx]
        code = row['code']
        buy_date = row['buy_date']
        label = int(row['label'])

        if code not in self.cache:
            return torch.zeros((LOOKBACK_WINDOW, FEATURE_SIZE)), torch.tensor(0.0)

        df = self.cache[code]

        try:
            target_mask = (df['æ—¥æœŸ'] == pd.to_datetime(buy_date))
            if not target_mask.any(): raise ValueError("Date not found")

            buy_idx = np.where(target_mask)[0][0]
            start_idx = buy_idx - LOOKBACK_WINDOW + 1
            if start_idx < 0: raise ValueError("Not enough history")

            slice_df = df.iloc[start_idx: buy_idx + 1]
            if len(slice_df) != LOOKBACK_WINDOW: raise ValueError("Length mismatch")

            vals = slice_df[['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æˆäº¤é‡']].values.astype(np.float32)
            base_price = vals[0, 0] if vals[0, 0] != 0 else 1e-6
            price_feats = vals[:, 0:4] / base_price - 1

            base_vol = np.mean(vals[:, 4])
            if base_vol == 0: base_vol = 1e-6
            vol_feat = vals[:, 4:5] / base_vol - 1

            features = np.hstack([price_feats, vol_feat])

            return torch.tensor(features), torch.tensor(label, dtype=torch.float32)

        except Exception:
            return torch.zeros((LOOKBACK_WINDOW, FEATURE_SIZE)), torch.tensor(0.0)


# ==========================================
# ğŸ§  2. æ¨¡å‹å®šä¹‰ (ç§»é™¤ Sigmoid)
# ==========================================
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        import math
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class NReboundTransformer(nn.Module):
    def __init__(self, input_size=5, d_model=64, nhead=4, num_layers=2, dropout=0.2):
        super(NReboundTransformer, self).__init__()
        self.embedding = nn.Linear(input_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        # batch_first=False æ˜¯é»˜è®¤å€¼ï¼Œä¿æŒç°çŠ¶å³å¯
        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        self.decoder = nn.Sequential(
            nn.Linear(d_model * LOOKBACK_WINDOW, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1),
            # ğŸ”¥ ä¿®æ”¹ï¼šç§»é™¤äº†æœ€åçš„ Sigmoidï¼
            # å› ä¸ºæˆ‘ä»¬è¦é…åˆ BCEWithLogitsLoss ä½¿ç”¨ï¼Œå®ƒå†…éƒ¨è‡ªå¸¦äº† Sigmoidï¼Œæ•°å€¼æ›´ç¨³å®š
        )

    def forward(self, src):
        src = src.permute(1, 0, 2)
        src = self.embedding(src)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = output.permute(1, 0, 2)
        output = output.reshape(output.size(0), -1)
        prob = self.decoder(output)
        return prob.squeeze()


# ==========================================
# ğŸ® 3. è®­ç»ƒä¸»æµç¨‹ (æ ¸å¿ƒæ”¹è¿›ç‰ˆ)
# ==========================================
def main():
    torch.set_float32_matmul_precision('medium')
    print(f"ğŸš€ å¯åŠ¨è®­ç»ƒå¼•æ“ (Proç‰ˆ) | æ˜¾å¡: {torch.cuda.get_device_name(0)}")

    if not os.path.exists(DATA_INDEX):
        print(f"âŒ æ‰¾ä¸åˆ° {DATA_INDEX}")
        return

    df = pd.read_csv(DATA_INDEX)

    # âš–ï¸ è®¡ç®—æ­£æ ·æœ¬æƒé‡
    pos_count = len(df[df['label'] == 1])
    neg_count = len(df[df['label'] == 0])
    pos_weight_val = neg_count / (pos_count + 1e-6)
    print(f"ğŸ“Š æ ·æœ¬åˆ†å¸ƒ: æ­£ {pos_count} / è´Ÿ {neg_count} | âš–ï¸ å»ºè®®æƒé‡: {pos_weight_val:.2f}")

    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])

    train_dataset = CachedStockDataset(train_df, RAW_DATA_DIR)
    val_dataset = CachedStockDataset(val_df, RAW_DATA_DIR)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,
                              persistent_workers=False)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, persistent_workers=False)

    model = NReboundTransformer().to(DEVICE)

    # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›1ï¼šä½¿ç”¨ BCEWithLogitsLoss å¹¶åŠ æƒ
    pos_weight_tensor = torch.tensor([pos_weight_val]).to(DEVICE)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›2ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨ (å¦‚æœ10ä¸ªepochæŒ‡æ ‡ä¸åŠ¨ï¼Œå­¦ä¹ ç‡å‡åŠ)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)

    best_precision = 0.0
    patience_counter = 0  # æ—©åœè®¡æ•°å™¨
    start_time = time.time()

    print(f"\n{'Epoch':<6} | {'Loss':<8} | {'Precision':<10} | {'Recall':<8} | {'LR':<8} | {'çŠ¶æ€'}")
    print("-" * 65)

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0

        for X, y in train_loader:
            # å¼ºåˆ¶è½¬ floatï¼Œé˜²æ­¢æŠ¥é”™
            X, y = X.to(DEVICE), y.to(DEVICE).float()

            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)

        # éªŒè¯
        model.eval()
        all_preds = []
        all_targets = []
        with torch.no_grad():
            for X, y in val_loader:
                X, y = X.to(DEVICE), y.to(DEVICE).float()
                output = model(X)
                # å› ä¸ºå»æ‰äº†Sigmoidï¼Œè¿™é‡Œè¦æ‰‹åŠ¨åŠ Sigmoidå†åˆ¤æ–­ > 0.5
                # æˆ–è€…ç›´æ¥åˆ¤æ–­ logits > 0 (æ•ˆæœä¸€æ ·)
                predicted = (torch.sigmoid(output) > 0.5).float()

                all_preds.extend(predicted.cpu().numpy())
                all_targets.extend(y.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        precision = precision_score(all_targets, all_preds, zero_division=0)
        recall = recall_score(all_targets, all_preds, zero_division=0)
        current_lr = optimizer.param_groups[0]['lr']

        # è¯„ä»·çŠ¶æ€
        status = ""
        if precision > 0.55:
            status = "ğŸ”¥ ä¼˜ç§€"
        elif precision > 0.50:
            status = "âœ… èµšé’±"
        elif precision > 0.45:
            status = "ğŸ”„ éœ‡è¡"
        else:
            status = "âŒ äºæŸ"

        print(
            f"{epoch + 1:<6} | {avg_loss:<8.4f} | {precision * 100:<9.1f}% | {recall * 100:<7.1f}% | {current_lr:<8.5f} | {status}")

        # è°ƒåº¦å™¨æ­¥è¿› (æ ¹æ® Precision è°ƒæ•´å­¦ä¹ ç‡)
        scheduler.step(precision)

        # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›3ï¼šåªæŒ‰ Precision ä¿å­˜æ¨¡å‹
        if precision > best_precision and precision > 0.5:  # åªæœ‰èƒœç‡>50%æ‰æœ‰ä¿å­˜ä»·å€¼
            best_precision = precision
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            patience_counter = 0  # é‡ç½®æ—©åœ
            status += " [ğŸ’¾ Saved]"
        else:
            patience_counter += 1

        # æ—©åœ
        if patience_counter >= 15:
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ 15 ä¸ª Epoch æ€§èƒ½æœªæå‡ã€‚")
            break

    total_time = (time.time() - start_time) / 60
    print(f"\nğŸ è®­ç»ƒç»“æŸï¼æ€»è€—æ—¶: {total_time:.1f} åˆ†é’Ÿ")
    print(f"ğŸ† æœ€ä½³æŸ¥å‡†ç‡ (Precision): {best_precision * 100:.2f}%")
    if best_precision > 0.5:
        print("âœ… æ¨¡å‹å¯ç”¨ï¼å¿«å»æ›¿æ¢ paper_bot é‡Œçš„æ¨¡å‹æ–‡ä»¶å§ï¼")
    else:
        print("âš ï¸ æ¨¡å‹æ•ˆæœä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–è°ƒæ•´ç‰¹å¾ã€‚")


if __name__ == "__main__":
    import multiprocessing

    multiprocessing.freeze_support()
    main()